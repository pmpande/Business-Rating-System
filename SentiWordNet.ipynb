{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json as js\n",
    "\n",
    "with open(\"/u/pmpande/author_profiling/training_file.json\") as f:\n",
    "    data = pd.DataFrame(js.loads(line) for line in f)\n",
    "f.close()\n",
    "\n",
    "with open(\"/u/pmpande/author_profiling/testing_file.json\") as ft:\n",
    "    data_test = pd.DataFrame(js.loads(line) for line in ft)\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "rand_train = random.sample(range(0,len(data)),50000)\n",
    "print(str(len(set(rand_train))))\n",
    "train = data.iloc[rand_train].copy()\n",
    "\n",
    "rand_test = random.sample(range(0,len(data_test)),5000)\n",
    "print(str(len(set(rand_test))))\n",
    "test = data_test.iloc[rand_test].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "corpus = []\n",
    "truth = []\n",
    "for row in train.iterrows():\n",
    "    corpus.append(row[1]['text'])\n",
    "    truth.append(row[1]['stars'])\n",
    "print(count)       \n",
    "count = 0\n",
    "corpus_test = []\n",
    "truth_test = []\n",
    "for row in test.iterrows():\n",
    "    corpus_test.append(row[1]['text'])\n",
    "    truth_test.append(row[1]['stars'])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "new = []\n",
    "count = 0\n",
    "for row in corpus:\n",
    "    lst = []\n",
    "    tokens = nltk.word_tokenize(row)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1] == 'JJ':\n",
    "            temp = (tag[0],'a')\n",
    "            lst.append(temp)\n",
    "        elif tag[1] == 'RB': \n",
    "            temp = (tag[0],'r')\n",
    "            lst.append(temp)\n",
    "\n",
    "        \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    score = []\n",
    "    for word in lst:\n",
    "        if len(list(swn.senti_synsets(word[0],word[1]))) != 0:\n",
    "            val = lesk(tokens, word[0])\n",
    "            word_sent = swn.senti_synset(val.name())\n",
    "            pos += word_sent.pos_score()\n",
    "            neg += word_sent.neg_score()\n",
    "    total = pos+neg\n",
    "    if total == 0:\n",
    "        score.append(0)\n",
    "        score.append(0)\n",
    "        score.append(1)\n",
    "    else:\n",
    "        score.append(pos/total)\n",
    "        score.append(neg/total)\n",
    "        obj_val = 1-((pos/total)+(neg/total))\n",
    "        if obj_val < 0:\n",
    "            obj_val *= -1\n",
    "        score.append(obj_val)\n",
    "    new.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_test = []\n",
    "for row in corpus_test:\n",
    "    lst = []\n",
    "    tokens = nltk.word_tokenize(row)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1] == 'JJ':\n",
    "            temp = (tag[0],'a')\n",
    "            lst.append(temp)\n",
    "        elif tag[1] == 'RB': \n",
    "            temp = (tag[0],'r')\n",
    "            lst.append(temp)\n",
    "        \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    score = []\n",
    "    for word in lst:\n",
    "        if len(list(swn.senti_synsets(word[0],word[1]))) != 0:\n",
    "            val = lesk(tokens, word[0])\n",
    "            word_sent = swn.senti_synset(val.name())\n",
    "            pos += word_sent.pos_score()\n",
    "            neg += word_sent.neg_score()\n",
    "            \n",
    "    total = pos+neg\n",
    "    if total == 0:\n",
    "        score.append(0)\n",
    "        score.append(0)\n",
    "        score.append(1)\n",
    "    else:\n",
    "        score.append(pos/total)\n",
    "        score.append(neg/total)\n",
    "        score.append(1-((pos/total)+(neg/total)))\n",
    "    new_test.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.array(new)\n",
    "test_x = np.array(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "clf = MultinomialNB().fit(train_x,truth)\n",
    "clf1 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42).fit(train_x,truth)\n",
    "clf2 = tree.DecisionTreeClassifier().fit(train_x,truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.421\n",
      "0.4306\n",
      "0.421\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(test_x)\n",
    "print(np.mean(predicted == truth_test))\n",
    "\n",
    "predicted1 = clf1.predict(test_x)\n",
    "print(np.mean(predicted1 == truth_test))\n",
    "\n",
    "predicted2 = clf2.predict(test_x)\n",
    "print(np.mean(predicted2 == truth_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00       602\n",
      "          2       0.00      0.00      0.00       424\n",
      "          3       0.00      0.00      0.00       620\n",
      "          4       0.00      0.00      0.00      1249\n",
      "          5       0.42      1.00      0.59      2105\n",
      "\n",
      "avg / total       0.18      0.42      0.25      5000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.32      0.52      0.40       602\n",
      "          2       0.00      0.00      0.00       424\n",
      "          3       0.17      0.06      0.09       620\n",
      "          4       0.00      0.00      0.00      1249\n",
      "          5       0.47      0.86      0.61      2105\n",
      "\n",
      "avg / total       0.26      0.43      0.32      5000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.34      0.38      0.36       602\n",
      "          2       0.18      0.05      0.08       424\n",
      "          3       0.23      0.04      0.07       620\n",
      "          4       0.24      0.09      0.13      1249\n",
      "          5       0.47      0.82      0.60      2105\n",
      "\n",
      "avg / total       0.35      0.42      0.34      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/pmpande/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "targetNames = ['1','2','3','4','5']\n",
    "print(metrics.classification_report(truth_test, predicted,target_names=targetNames))\n",
    "print(metrics.classification_report(truth_test, predicted1,target_names=targetNames))\n",
    "print(metrics.classification_report(truth_test, predicted2,target_names=targetNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0  602]\n",
      " [   0    0    0    0  424]\n",
      " [   0    0    0    0  620]\n",
      " [   0    0    0    0 1249]\n",
      " [   0    0    0    0 2105]]\n",
      "---------------------------------\n",
      "[[ 312    0   34    0  256]\n",
      " [ 136    0   39    0  249]\n",
      " [ 144    0   37    0  439]\n",
      " [ 144    0   42    0 1063]\n",
      " [ 234    0   67    0 1804]]\n",
      "---------------------------------\n",
      "[[ 229   23   15   43  292]\n",
      " [  90   21   17   59  237]\n",
      " [  97   22   25   68  408]\n",
      " [  97   28   24  114  986]\n",
      " [ 160   20   26  183 1716]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(truth_test, predicted))\n",
    "print(\"---------------------------------\")\n",
    "print(metrics.confusion_matrix(truth_test, predicted1))\n",
    "print(\"---------------------------------\")\n",
    "print(metrics.confusion_matrix(truth_test, predicted2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8534832073693033\n",
      "1.7545939701252822\n",
      "1.7175564037317668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "print(sqrt(mean_squared_error(truth_test, predicted)))\n",
    "print(sqrt(mean_squared_error(truth_test, predicted1)))\n",
    "print(sqrt(mean_squared_error(truth_test, predicted2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
